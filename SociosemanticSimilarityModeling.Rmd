---
title: "SociosemanticSimilarityModeling"
author: "Jean-Francois Chartier"
date: "1 mai 2019"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Load packages
```{r}
library(magrittr)
library(quanteda)
quanteda::quanteda_options("threads" = 6)
library(data.table)
library(proxy)
library(text2vec)

```

#functions
```{r}
#function to create a unit normed vector
normVector <- function(x) 
{
  if(sum(x)==0)
    return (x)
  else 
    return (x / sqrt(sum(x^2)))
  
}
#function to norm many vectors
normRowVectors<-function(m){
  t(apply(m, MARGIN = 1, FUN = function(x) normVector(x)))
}


```


#read data
```{r}
agent.enron = readRDS("agent.enron.rds")
content.enron=readRDS("content.enron.rds")
interaction.enron=readRDS("interaction.enron.rds")  
```

#Aggregate messages by sender 

```{r}
map.content.sender=unique(interaction.enron[,1:2])
idContentBySender=stats::aggregate(map.content.sender, by=list(map.content.sender$id.sender), FUN=function(x){unique(c(x))}) 

tokensBySender=lapply(idContentBySender$id.content, function(l){
  
  c(content.enron$clean.content.token[unlist(l)])%>%unlist(.)
})


```



#Get number of messages sent by agent
not used
```{r}
# sender.freq.feature=apply(idContentBySender, MARGIN = 1, function(x){
#   data.frame(agent.id=x$id.sender, n.message=length(x$id.content), n.receiver=length(x$id.receiver))
#   })%>%data.table::rbindlist(.)
```

#Sociosemantic modeling

##Build basic document*term frequency matrix
```{r}
#need to encode as quanteda's tokens format before building the matrix
sender.term.matrix = quanteda::dfm(x=as.tokens(tokensBySender), tolower=FALSE)

print(c("vocabulary size:" ))
length(sender.term.matrix@Dimnames$features)
```
##filter too rare and too frequent words
```{r}
#threshold set in order to have a matrix of size that fit in the RAM
#a matrix with more than 20k dimensions is very hard to handle
min.thres=20
max.thres=length(sender.term.matrix@Dimnames$features)*.5
sender.term.matrix.2=quanteda::dfm_trim(x=sender.term.matrix, min_docfreq = min.thres, max_docfreq = max.thres, docfreq_type="count")
print(c("vocabulary size:" ))
length(sender.term.matrix.2@Dimnames$features)

#change doc feature by id.sender for further convenient retrieval 
sender.term.matrix.2@Dimnames$docs=idContentBySender$id.sender
```

##get non-empty context vector
empty vector can not be used in some semantic model. Therefore they must be filtered out
```{r}
# nonEmptyVectors = apply(X = as.matrix(sender.term.matrix.2), MARGIN = 1, FUN = function(x) sqrt(sum(x^2))>0)
# 
# print(paste("how many empty vectors: ", length(sender.term.matrix.2@Dimnames$docs[!nonEmptyVectors])))

```

##Add dummy
if the matrix has empty vectors, add un dummy variable
```{r}
# dummy=data.frame(dummy.feature=c(rep(1, nrow(sender.term.matrix.2))))
# 
# sender.term.matrix.3=sender.term.matrix.2%>%as.matrix(.)%>%cbind(., dummy)%>%as.dfm(.)
# 
# nonEmptyVectors.3 = apply(X = as.matrix(sender.term.matrix.3), MARGIN = 1, FUN = function(x) sqrt(sum(x^2))>0)
# print(paste("how many non empty vectors: ", nrow(sender.term.matrix.3[nonEmptyVectors.3,])))

```


#Explicit Semantic Modeling


##Binary similarity modeling
the binary similarity is computed with the Jaccard coefficient
use sim2 from text2vec. The fastest similarity computation found so far.
Much more faster than the package proxy

```{r}
sender.term.matrix.bin  = quanteda::dfm_weight(sender.term.matrix.2, scheme  = "boolean")
bin_simil_members=text2vec::sim2(sender.term.matrix.bin, method = "jaccard", norm = "none")

bin_simil_members=as.matrix(bin_simil_members)

```
###save
```{r}
saveRDS(bin_simil_members, file = "enron.binary.sim.model.rds")

```

##Normalised Frequency modeling

```{r}
sender.term.matrix.prop=quanteda::dfm_weight(sender.term.matrix.2, scheme  = "prop")

prop.simil.member=text2vec::sim2(sender.term.matrix.prop, method = "cosine", norm="none")%>% as.matrix(.)

```
###Save
```{r}
saveRDS(prop.simil.member, file="enron.prop.sim.member.rds")
```


##TF-IDF modeling
```{r}
sender.term.matrix.prop  = quanteda::dfm_tfidf(sender.term.matrix.2, scheme_tf  = "prop", scheme_df="inverseprob")

tfidf.simil.member=text2vec::sim2(sender.term.matrix.prop, method = "cosine", norm="none")%>% as.matrix(.)

```

###save
```{r}
saveRDS(tfidf.simil.member, file="enron.tfidf.sim.member.rds")
```

#Latent Semantic Modeling 


##Build message*term matrix
Latent models are build with a segment*term matrix. The model is then aggregated by sender. For example, the topic distribution of a every text segments sended by a agent will be averaged. Same thing for LSA 
```{r}
#need to encode as quanteda's tokens format before building the matrix
message.term.matrix = quanteda::dfm(x=as.tokens(content.enron$clean.content.token), tolower=FALSE)

print(c("vocabulary size:" ))
length(message.term.matrix@Dimnames$features)

message.term.matrix.filtered=quanteda::dfm_trim(x=message.term.matrix, min_docfreq = 75, max_docfreq = length(message.term.matrix@Dimnames$features)*.3, docfreq_type="count")

#message.term.matrix.filtered.withEmpty=quanteda::dfm_trim(x=message.term.matrix, min_docfreq = 75, max_docfreq = length(message.term.matrix@Dimnames$features)*.5, docfreq_type="count")

print(c("vocabulary size:" ))
length(message.term.matrix.filtered@Dimnames$features)

#add dummy feature to avoid empty vector
# message.term.matrix.filtered=data.frame(dummy.feature=c(rep(1, nrow(message.term.matrix.filtered))))%>%as.matrix(.)%>%cbind(., message.term.matrix.filtered)%>%as.dfm(.)
```



##LDA-based topic modeling
used the word2vec package
http://text2vec.org/topic_modeling.html#latent_dirichlet_allocation
Compared to the "topicmodels" package, text2vec LDA is much more faster 

Note that the text2vec LDA can handle empty vectors (after word filtering) while the topicmodels version can not.
After many trials, doc_topic_prior = 0.1 and topic_word_prior = 0.01 seems to be a good educated guess for parameters estimations, but cross-validation should done to be sure. 
```{r}
#version with empty vectors
lda.model.message.k100.tp001 = text2vec::LDA$new(n_topics = 100, doc_topic_prior = 0.1, topic_word_prior = 0.001)

message.topic.distr = lda.model.message.k100.tp001$fit_transform(x = message.term.matrix.filtered, n_iter = 1000, convergence_tol = 0.001, n_check_convergence = 10, progressbar = FALSE)

```

###save 
```{r}
saveRDS(message.topic.distr, "message.topic.distr.rds")
saveRDS(lda.model.message.k100.tp001, "lda.model.message.k100.tp001.rds")
```
###plot topics
use the LDAvis' method
```{r}
lda.model.message.k100.tp001$plot()
```


##aggregate topic distribution by senders
```{r}
agg.topic.distr.BySender=stats::aggregate(message.topic.distr, by=list(map.content.sender$id.sender), mean) 

```

###save 
```{r}
saveRDS(agg.topic.distr.BySender, "LDA.agg.topic.distr.BySender.rds")
```

##Similarity calculation
do not keep the first colum, as it corresponds to the id.senders
```{r}
agg.topic.distr.BySender=readRDS("LDA.agg.topic.distr.BySender.rds")
lda_simil_members=text2vec::sim2(as.matrix(agg.topic.distr.BySender[,2:ncol(agg.topic.distr.BySender)]), method = "cosine", norm = "none")%>% set_rownames(agg.topic.distr.BySender$Group.1) %>% set_colnames(agg.topic.distr.BySender$Group.1)
```

###save
```{r}
saveRDS(lda_simil_members, file="enron.lda.sim.member.rds")
```


##LSA-based topic modeling
```{r}
set.seed(1)

mySVD=irlba::irlba(message.term.matrix.filtered%>%dfm_weight(x=., scheme="prop"), 100, tol=1e-5)

latentNormedMessageSpace = as.matrix(mySVD$u %*% solve(diag((mySVD$d)))) %>% normRowVectors()

```
###save
```{r}
saveRDS(mySVD, "lsa.model.message.k100.rds")
```

##aggregate topic distribution by senders
```{r}
agg.latentDim.BySender=stats::aggregate(latentNormedMessageSpace, by=list(map.content.sender$id.sender), mean) 

```

###save 
```{r}
saveRDS(agg.latentDim.BySender, "LSA.agg.latentDim.distr.BySender.rds")
```

##Similarity calculation
```{r}
#agg.latentDim.BySender=readRDS("LSA.agg.latentDim.distr.BySender.rds")
lsa_simil_members=text2vec::sim2(as.matrix(agg.latentDim.BySender[,2:ncol(agg.latentDim.BySender)]), method = "cosine", norm = "none") %>% set_rownames(agg.latentDim.BySender$Group.1) %>% set_colnames(agg.latentDim.BySender$Group.1)
```

###save
```{r}
saveRDS(lsa_simil_members, file="enron.lsa.sim.member.rds")
```

##NNMF-based topic modeling
non-negative matrix factorization k=100
nnmf does not work if there is empty vectors, so we need to add a dummy feature

2 libraries were tested. The 2 were super slow. NMF does not take sparse matrix, and a dense matrix is too large for a normal computer. nnTensor seems like a good alternative, but is also very slow. So far, no successfull solution has been found.
```{r}
# library(nnTensor)
# nnmf.model=nnTensor::NMF(message.term.matrix.filtered%>%dfm_weight(x=., scheme="prop"), J = 100)
# 
# library(NMF)
# dummy=c(dummy.feature=c(rep(1, nrow(message.term.matrix.filtered))))
# 
# message.term.matrix.filtered.wd=message.term.matrix.filtered%>%Matrix::cBind(., dummy)%>%as.dfm(.)
# 
# #i0 <- which(colSums(message.term.matrix.filtered.wd) == 0)
# 
# nnmf.model.2=NMF::nmf(message.term.matrix.filtered.wd[1:100,]%>%dfm_weight(x=., scheme="prop") %>% as.matrix(.), rank=2, method = "brunet", .options=list('v'=T, 'P'=T), seed=12345)
```


