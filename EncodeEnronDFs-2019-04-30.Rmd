---
title: "PreprocessEnronCorpus"
author: "Jean-Francois Chartier"
date: "30 avril 2019"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#packages
```{r}
library(magrittr)
library(stringr)
library(quanteda)
quanteda::quanteda_options("threads" = 6)
library(data.table)
```


# Get dataset
The Enron email dataset contains approximately 500,000 emails generated by employees of the Enron Corporation. It was obtained by the Federal Energy Regulatory Commission during its investigation of Enron's collapse.
The data set was collected from https://github.com/brianray/data.world-scripts/blob/master/enron_email_script.ipynb.
This is so far the most well structured version of the corpus I have found. 

```{r}
myEnronFile="enron_05_17_2015_with_labels_v2.csv"
enronData = read.csv(file=myEnronFile, header = TRUE, sep = ",", encoding = "UTF-8", strip.white=TRUE, stringsAsFactors = F)

#select only relevant columns in the data set for our needs
enronData=enronData[, 1:15]

```

#1. Create Agent DF

Create a dataframe with all unique emails indexed and their role in the data set

##1.1 extract all unique email address senders
```{r}
#function that can be reused
extract.email.from<- function(list.enron.senders){
  result = stringr::str_match_all(list.enron.senders, pattern = "\\{'(.*?)'\\}") %>% lapply(., function(x) x[,2]) %>% stringr::str_split(., ",") %>% unlist(.) %>%unique(.)
  return (result)
  }

#emailFromSenders = stringr::str_match_all(enronData$From, pattern = "\\{'(.*?)'\\}") %>% lapply(., function(x) x[,2]) %>% stringr::str_split(., ",") %>% unlist() %>%unique()

emailFromSenders=extract.email.from(enronData$From)
```

##1.2 extract all unique email adress receivers
```{r}
#function that can be reused
extract.email.to<-function(list.enron.receivers){
  stringr::str_match_all(list.enron.receivers, pattern = "\\{(.*?)\\}") %>% lapply(., function(x) x[,2]) %>% stringr::str_match_all(., pattern = "'(.*?)'") %>% lapply(., function(y) y[,2]) %>% unlist() %>%unique()%>%return(.)
}

emailFromReceivers=extract.email.to(enronData$To)
```

##1.3 merge senders and receivers
```{r}
email.adress = union(tolower(emailFromSenders), tolower(emailFromReceivers))

```


##1.4 check suspicious email adresses 
this script can be used to identify incorrect email adresses. The preprocessing of email tokens could be investigated further. Here I only keep track of potentially suspicious emails  

Note that more than 2000 emails are suspicious, but it is relatively small in comparison to all indexed emails (i.e. 76,334)   
```{r}

#check email with empty space within
withEmpty = vapply(email.adress, function(x) {
  stringi::stri_detect_fixed(x , pattern = " ")
}, FUN.VALUE = logical(1))


```


##1.5 Create data frame of agent.enron 
```{r}
is.sender = email.adress %in% emailFromSenders
is.receiver = email.adress %in% emailFromReceivers
agent.enron=data.frame(email.adress=email.adress, is.sender=is.sender, is.receiver=is.receiver, is.suspicious = withEmpty, stringsAsFactors = F)

```
###save data.frame
```{r}
saveRDS(agent.enron, "agent.enron.rds")
```


#2. Create Message DF

Create a dataframe with all contents from Enron's emails.

##2.1 Extract email content
```{r}
raw.content = enronData$content

#quick clean of text
#remove all non graphical caracther
preprocesCorpus=stringr::str_replace_all(raw.content,"[^[:graph:]]", " ")
#remove whitespace
preprocesCorpus=stringr::str_squish(preprocesCorpus)
#tokenize
preprocesCorpus=quanteda::tokens(x=preprocesCorpus,what="word", remove_punct = TRUE, remove_numbers = TRUE, remove_separators = TRUE,remove_hyphens = TRUE, remove_symbols=TRUE, remove_url = TRUE)
preprocesCorpus=quanteda::tokens_tolower(preprocesCorpus)


myStopWords=unique(c(stopwords("en", source = "smart")))

# filter with the stopwords and singleton
preprocesCorpus=quanteda::tokens_remove(preprocesCorpus, case_insensitive = T, valuetype = "glob", pattern=myStopWords, min_nchar=3)

content.enron=data.frame(raw.content=raw.content, date=enronData$Date, stringsAsFactors = F)
#somehow, we can only add the colum after the df was created
clean.content.token=as.list(unname(preprocesCorpus))

content.enron$clean.content.token=clean.content.token
```

###Save
```{r}
saveRDS(content.enron, "content.enron.rds")
```


#3. Create Interraction DF
```{r}

agent.enron = readRDS("agent.enron.rds")
#set key to fastenen sorting operation
agent.enron=data.table::as.data.table(agent.enron) %>%setkey(.,email.adress)


interaction.enron=lapply((1: nrow(content.enron)), function(i){
  to=extract.email.to(enronData$To[i])
  #if empty
  if (length(to)==0){
    to=NA
  } else{
    #ge row index in agent.enron dataframe
    to = sapply(to, function(x){
      agent.enron[email.adress==x, which=T]
    })
  
  }
   
  #check if empty email
  # sapply((1:length(to)), function(j){
  #   if (length(to[j])<1)
  #     to[j]=NA
  # })
  # 
  from=extract.email.from(enronData$From[i])
  if (length(from)==0){
    from=NA
  } else{
    from=agent.enron[email.adress==from, which=T]%>% rep(., (length(to)))
  }
  id.c=rep(i, (length(to)))
  
  data.frame(id.content=id.c, id.sender=from, id.receiver =to, stringsAsFactors = F)
  
})%>%data.table::rbindlist(.)
  
  
```

###save
```{r}
saveRDS(interaction.enron, "interaction.enron.rds")  
```
###########################
#DEPRACATED

#3.a Alternative Create Interraction DF
```{r}
start_time <- Sys.time()

#agent.enron = readRDS("agent.enron.rds")

#list.receivers = stringr::str_match_all(enronData$To, pattern = "\\{(.*?)\\}") %>% lapply(., function(x) x[,2]) %>% stringr::str_match_all(., pattern = "'(.*?)'") %>% lapply(., function(y) y[,2]) %>% unlist()


#interaction.enron = data.frame(id.content=integer(), id.sender = integer(), id.receiver=integer())


n.receivers = vector(mode="list", length = length(enronData$To))

to=lapply(enronData$To, function(x) extract.email.to(x))
n.receivers=lapply(to, function(x) length(x))
id.receiver=lapply(to, function(x){
  sapply(x, function(y) which(agent.enron$email.adress==y), USE.NAMES = F)
})


id.receiver2=lapply(to, function(x){
  sapply(x, function(y) grep(pattern = y, agent.enron$email.adress, fixed = T), USE.NAMES = F)
})

id.receiver3=lapply(to[1:1000], function(x){
  if(length(x)==1){
    grep(pattern = x, agent.enron$email.adress, fixed = T)
  }
  else{
    sapply(x, function(y) grep(pattern = y, agent.enron$email.adress, fixed = T), USE.NAMES = F)
  }
})


id.content=vector(mode = "list", length = length(id.receiver))
id.sender = vector(mode = "list", length = length(id.receiver))

for (i in 1: length(enronData$content))
  {
  #print(i)
  #extract.email.to(enronData$To[i])%>%lapply(., function(x) grep(x, agent.enron$email.adress, fixed = T)) %>% unlist(.) -> to
  #id.receiver[[i]]=to

  #extract.email.to(enronData$To[i])%>%lapply(., function(x) which(agent.enron$email.adress==x)) %>% unlist(.) -> to
  #id.receiver[[i]]=to

  #extract.email.from(enronData$From[i])%>% grep(., agent.enron$email.adress, fixed = T) %>% rep(., n.receivers[i]) -> from


  #start_time <- Sys.time()
  #extract.email.from(enronData$From[10000])%>% stringi::stri_locate_first_fixed(agent.enron$email.adress, pattern = .)[,1] %>% rep(., n.receivers[i]) -> from1
 # end_time <- Sys.time()
  #print(end_time - start_time)




  #extract.email.from(enronData$From[i]) %>% which(agent.enron$email.adress) %>% rep(., (length(to))) -> from


  x= extract.email.from(enronData$From[i])
  id=which(agent.enron$email.adress==x)
  from = rep(id, n.receivers[i])


  id.sender[[i]]=from

  #id.content[[i]]=rep(i, n.receivers[i])

  }


end_time <- Sys.time()
print(end_time - start_time)

```

#3.b Alternative Create Interraction DF
```{r}
start_time <- Sys.time()

agent.enron = readRDS("agent.enron.rds")

to=lapply(enronData$To, function(x) extract.email.to(x))
n.receivers=lapply(to, function(x) length(x))
id.receiver=lapply(to, function(x){
  sapply(x, function(y) which(agent.enron$email.adress==y), USE.NAMES = F)
})



id.content=vector(mode = "list", length = length(id.receiver))
id.sender = vector(mode = "list", length = length(id.receiver))

for (i in 1: length(enronData$content))
  {

  x= extract.email.from(enronData$From[i])
  id=which(agent.enron$email.adress==x)
  from = rep(id, n.receivers[i])
  id.sender[[i]]=from
  id.content[[i]]=rep(i, n.receivers[i])
  }

end_time <- Sys.time()
print(end_time - start_time)

interaction.enron = data.frame(id.content=unlist(id.content), id.sender = unlist(id.sender), id.receiver=unlist(id.receiver))

saveRDS(interaction.enron, "interaction.enron.rds")
```





#preprocess text

```{r}
source("CorpusPreprocessingFunctions.R")

preproText = preprocessingCorpus(enronData$content)
preproText=quanteda::tokens_remove(preproText, case_insensitive = TRUE, valuetype = "fixed", pattern=stopwords("en")) %>% quanteda::tokens_wordstem(., language = "en")

df_textAndEmail = data.frame(senderEmail=emailFromSenders, stringsAsFactors = F)
df_textAndEmail$Text= quanteda::as.list(preproText)
```




# get relations sender-receiver
```{r}
listReceivers = stringr::str_match_all(enronData$To, pattern = "\\{'(.*?)'\\}") %>% lapply(., function(x) x[,2]) %>% stringr::str_split(., ",") %>% lapply(., function(s) stringr::str_trim(s, side = "both"))

uniqueReceivers = stringr::str_match_all(enronData$To, pattern = "\\{'(.*?)'\\}") %>% lapply(., function(x) x[,2]) %>% stringr::str_split(., ",") %>% unlist() %>%unique()

uniqueSender = unique(emailFromSenders)

intersecSenderReceiver = intersect(uniqueSender, uniqueReceivers)

```

#count interactions
```{r}

numberMessage = vapply(intersecSenderReceiver, function(x) {
  vapply(enronData$To, FUN = function(x){

  })
  stringi::stri_detect_fixed(str = enronData$To, pattern = x)%>% sum(.)
}, FUN.VALUE = integer(1))

#create dataframe for sender-receiver
df_senderRelation = data.frame(sender=intersecSenderReceiver, receiver = intersecSenderReceiver, frequency = numberMessage)

```



#aggragate text by email
very slow
```{r}

textByEmail=sapply(emailFromSenders, FUN = function(x) {
  stringr::str_detect(enronData$From, pattern = x) %>% enronData$content[.]
})

```


#aggragate text by email
fast
```{r}

#textByEmail=stats::aggregate(enronData$content, by=list(enronData$From), paste, collapse=" ")

textByEmail=stats::aggregate(enronData$content, by=list(emailFromSenders), paste, collapse=" ")

colnames(textByEmail)=c("email", "aggregatedText")
```



```{r}
source("CorpusPreprocessingFunctions.R")

preproTextByEmail = preprocessingCorpus(textByEmail$aggregatedText)

```

#stopwords and stemming
```{r}
preproTextByEmail=quanteda::tokens_remove(preproTextByEmail, case_insensitive = TRUE, valuetype = "fixed", pattern=stopwords("en")) %>% quanteda::tokens_wordstem(., language = "en")
```








